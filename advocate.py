# -*- coding: utf-8 -*-
"""Advocate.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H6pHbv4W2M2ZMKZKMjwr3mslYZzBOkFc

# Whisky Advocate
## *Insights from Reviews*

What makes a great whiskey? It is a smoky flavor? It is peaty? Is it something else?

There isn't a single truth of a great whisky, because tastes vary so much from person to person. However, we can analyze the reviews of experts to see if we can detect patterns in the flavor profiles of 'great' whiskys.

Today, you'll see the following things operationalized in python to study great whiskys:
* [Web Scraping](#web)
* [Data Exploration](#explore)
* [Machine Learning](#ml)
* [Data Visualization](#viz)

<a id="web"></a>
## Web Scraping
To gather our review data to understand great whiskeys, we will *scrape* [Whisky Advocate's](http://whiskeyadvocate.com) over 4,00 reviews. We will use python to access the reviews on the web and extract the relevant information from directly from the site's `html`.
"""

import pandas as pd
import requests
from bs4 import BeautifulSoup

score_range = ['95-100', '90-94', '80-89', '70-79', '60-69']

url = "http://whiskyadvocate.com/ratings-reviews/?search=&submit=&brand_id=0&rating={}&price=0&category=0&styles_id=0&issue_id=0"

urls = [url.format(score) for score in score_range]

tags = ['ratingValue', 'name', 'category', 'priceCurrency', 
 'price', 'description', 'author']

def get_review(review):
    data = {}
    for t in tags:
        data[t] = review.find(itemprop=t).text

    return data

def whiskey(urls):
    
    whiskey_reviews = list()
    
    for u in urls:
        r = requests.get(u)
        
        soup = BeautifulSoup(r.text, "lxml")
        ratings = soup.find(class_="ratings-grid-holder")
        
        ratings = ratings.find_all("article")
        
        reviews = [get_review(r) for r in ratings]
        
        whiskey_reviews += reviews
    
    return whiskey_reviews

data = whiskey(urls)

data[42]

"""<a id="explore"></a>
## Data Exploration
"""

df = pd.DataFrame.from_records(data)

df.head()

df.shape[0]

df['ratingValue'].astype('int').describe()

df['category'].value_counts()

df['price'].head()

df['p2'] = df['price'].astype('str').replace(",","").replace("$", "")

df['p2'].head()

df['p2'].str.replace("$", "").str.replace("/set","").astype('str')

df['p2'] = pd.to_numeric(df['p2'], errors='coerce')

df['price'] = pd.to_numeric(df['price'], errors='coerce')

df['price'].sort_values( ascending=False)

import seaborn as sns
from locale import atof

#df['price'] = df['price'].apply(atof)
sns.regplot(x=df[df['price'] < 20000]['ratingValue'].astype('float'), y=df[df['price'] < 20000]['price'])

df['pert_alcohol'] = df['name'].apply(lambda x: x.split()[-1][:-1])

df['pert_alcohol'].head()

df['pert_alcohol'] = pd.to_numeric(df['pert_alcohol'], errors='coerce')

df['pert_alcohol'].head()

def  is_it_perct(name):
    #print(name)
    last_seg = name.split()[-1]
    if last_seg[-1:] == "%":
        return last_seg[:-1]
    else:
        return None

df.loc[df['name'].isnull(), 'name'] = 'Bunnahabhain'

df['pert_alcohol'] = df['name'].apply(is_it_perct)
df['pert_alcohol'] = pd.to_numeric(df['pert_alcohol'], errors='coerce')

df['pert_alcohol'].head()

df.head()

df['pert_alcohol'].describe()

import seaborn as sns
sns.distplot(df[-df['pert_alcohol'].isnull()]['pert_alcohol'], bins=30)

import numpy as np
df['ratingValueLn'] = np.log(df['ratingValue'].astype('float'))

sns.distplot(df['ratingValueLn'].astype('float'), bins=10)

from sklearn.preprocessing import MaxAbsScaler

scaler = MaxAbsScaler()

x = df['ratingValue'].values

x =x.reshape(-1, 1)

df['ratingValueScaled'] = scaler.fit_transform(x)

df['ratingValueScaled'] = df['ratingValueScaled']

sns.distplot(df['ratingValue'].astype('float'), bins=10)

sns.distplot(df['ratingValueScaled'].astype('float'), bins=10)

import numpy as np
import matplotlib.pyplot as plt

types = df['category'].value_counts()[:9]

height = types.tolist()
bars = types.index.tolist()
y_pos = np.arange(len(bars))

plt.bar(y_pos, height)
plt.xticks(y_pos, bars, rotation='vertical')

plt.show()

list(df)

#df.to_csv('whiskey_reviews.csv')

"""<a id="ml"></a>
## Machine Learning
* Can we use the words of the reviews to predict a whisky's review score? 
* Which words are the most important / indicate a good review? 

We'll be applying linear regression to help us answer these questions. To process the text, we'll be using a text vectorization method: Tf-idf (Term Frequency - Inverse Document Frequency).
"""

from sklearn.feature_extraction.text import TfidfVectorizer

vect = TfidfVectorizer(stop_words='english', max_features=1000, 
                       min_df=5, max_df=.8, ngram_range=(1,2))
X = vect.fit_transform(df['description'])

X.todense()

vect.get_feature_names()[60:80]

from sklearn.linear_model import LinearRegression

reg = LinearRegression()
reg.fit(X, df['ratingValue'].astype('float'))

from sklearn.metrics import mean_squared_error, r2_score

y_pred = reg.predict(X)
print(r2_score(df['ratingValue'].astype('float'), y_pred))

model = pd.DataFrame(reg.coef_, columns=['coef'], index=vect.get_feature_names())

y = df['ratingValue'].astype('float').tolist()

from scipy import stats
params = np.append(reg.intercept_,reg.coef_)
predictions = y_pred
newX = pd.DataFrame({"Constant":np.ones(len(X.todense()))}).join(pd.DataFrame(X.todense()))
MSE = (sum((y-predictions)**2))/(len(newX)-len(newX.columns))

# Note if you don't want to use a DataFrame replace the two lines above with
# newX = np.append(np.ones((len(X),1)), X, axis=1)
# MSE = (sum((y-predictions)**2))/(len(newX)-len(newX[0]))

var_b = MSE*(np.linalg.inv(np.dot(newX.T,newX)).diagonal())
sd_b = np.sqrt(var_b)
ts_b = params/ sd_b

p_values =[2*(1-stats.t.cdf(np.abs(i),(len(newX)-1))) for i in ts_b]

sd_b = np.round(sd_b,3)
ts_b = np.round(ts_b,3)
p_values = np.round(p_values,3)
params = np.round(params,4)

model["Std Error"], model["t values"], model['p values'] = [sd_b[1:], ts_b[1:], p_values[1:]]

model.sort_values(by=[ 'coef', 'p values'])

"""<a id="viz"></a>
## Data Vizualization

We had a 1,000 features go into our regression model. We can only visualize a handful before the vizualization becomes overwhelming. We going to subset to only the features that are statistically signicant and then sample only 20 of those features. 
"""

sample = model[model['p values'] <= 0.01].sample(20).sort_values(by='coef')
sample

"""### Regression Intercept
If reviewer wrote nothing in the review, then the whiskey's review score would be 80.5 based on our model. Each word in the review then adds or subtracts from the score based on coefficient associated with the term. 

Remeber here, that's it's not the count of the word's presence in the review. The value that you multiple the coefficient by is the tf-idf score of the term or phrase in that particular review.
"""

print("Baseline whisky review score: {}".format(reg.intercept_))

from bokeh.io import show, output_notebook
from bokeh.plotting import figure
from bokeh.models import HoverTool

output_notebook()

p = figure(x_range=sample.index.tolist(), plot_height=350, 
           title="Whiskey Review Words", tools='hover')
p.vbar(x=sample.index.tolist(), top=sample['coef'], width=0.8)
p.xaxis.major_label_orientation = "vertical"
hover = p.select(dict(type=HoverTool))
hover.tooltips = [('Word',' @x'), ('Coef', ' @top')]
show(p)

